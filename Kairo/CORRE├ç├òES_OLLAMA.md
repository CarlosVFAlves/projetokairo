# ğŸ”§ CORREÃ‡Ã•ES PARA COMUNICAÃ‡ÃƒO COM OLLAMA

## âŒ PROBLEMAS IDENTIFICADOS

### 1. **Erro no Template de Prompt**
**Sintoma:** `Erro ao gerar prompt: '\n  "internal_monologue"'`
**Causa:** FormataÃ§Ã£o incorreta das chaves no template JSON

### 2. **Respostas NÃ£o Estruturadas**
**Sintoma:** Ollama retorna texto simples em vez de JSON
**Causa:** Falta de instruÃ§Ãµes especÃ­ficas para formato JSON

### 3. **Respostas GenÃ©ricas de Fallback**
**Sintoma:** Kairo responde sempre "Desculpe, nÃ£o posso ajudar"
**Causa:** Sistema usa fallback genÃ©rico quando nÃ£o consegue processar JSON

## âœ… CORREÃ‡Ã•ES APLICADAS

### 1. **Template de Prompt Corrigido**
```python
# ANTES (problemÃ¡tico):
"internal_monologue": "texto"

# DEPOIS (corrigido):
{{"internal_monologue": "texto"}}  # Escape duplo das chaves
```

### 2. **ForÃ§ar Formato JSON no Ollama**
```python
payload = {
    "model": model_to_use,
    "prompt": f"{prompt}\n\nIMPORTANTE: Responda APENAS com JSON vÃ¡lido.",
    "format": "json",  # â† NOVO: ForÃ§a formato JSON
    "options": {
        "temperature": 0.7,
        "stop": ["\n\n", "```"]  # â† NOVO: Para em quebras
    }
}
```

### 3. **ExtraÃ§Ã£o de JSON Melhorada**
- âœ… Tenta parse direto primeiro
- âœ… Busca JSON entre chaves se necessÃ¡rio
- âœ… ValidaÃ§Ã£o mais flexÃ­vel (parameter opcional)
- âœ… Melhor tratamento de erros

### 4. **Fallback Inteligente**
- âœ… Remove prefixos desnecessÃ¡rios
- âœ… Limita tamanho da resposta
- âœ… Resposta padrÃ£o se texto muito curto
- âœ… Melhor interpretaÃ§Ã£o do texto bruto

## ğŸ¯ RESULTADO ESPERADO

**ANTES:**
```
ğŸ‘¤ VocÃª: Qual a capital do Brasil?
ğŸ¤– Kairo: Desculpe, nÃ£o posso ajudar com essa pergunta.
```

**DEPOIS:**
```
ğŸ‘¤ VocÃª: Qual a capital do Brasil?
ğŸ¤– Kairo: A capital do Brasil Ã© BrasÃ­lia! Ã‰ uma cidade planejada, construÃ­da especificamente para ser a capital do paÃ­s.
```

## ğŸ”§ CONFIGURAÃ‡Ã•ES RECOMENDADAS DO OLLAMA

### **Modelos Testados e Funcionais:**
```bash
# Melhor para conversaÃ§Ã£o:
ollama pull llama2:7b

# Alternativa rÃ¡pida:
ollama pull llama2:3b

# Para mÃ¡quinas potentes:
ollama pull llama2:13b
```

### **ConfiguraÃ§Ã£o no config.py:**
```python
OLLAMA_CONFIG = {
    "model": "llama2:7b",  # â† Use este modelo
    "temperature": 0.7,    # â† Criatividade balanceada
    "max_tokens": 1000     # â† Respostas completas
}
```

## ğŸš€ COMO TESTAR

### 1. **Verificar Sistema:**
```bash
cd kairo/maestro
python verificacao_completa.py
# Deve mostrar: "8/10 mÃ³dulos funcionando"
```

### 2. **Testar Ollama:**
```bash
ollama serve
ollama run llama2:7b "Responda em JSON: {'resposta': 'teste'}"
```

### 3. **Executar Kairo:**
```bash
python main.py
```

### 4. **Testar ConversaÃ§Ã£o:**
```
ğŸ‘¤ VocÃª: OlÃ¡, quem Ã© vocÃª?
ğŸ¤– Kairo: [Deve responder normalmente agora]

ğŸ‘¤ VocÃª: Qual a capital do Brasil?
ğŸ¤– Kairo: [Deve responder corretamente]
```

## âš ï¸ TROUBLESHOOTING

### **Se ainda der erro de JSON:**
1. Verifique se o modelo estÃ¡ correto: `ollama list`
2. Teste o modelo diretamente: `ollama run llama2:7b`
3. Reinicie o Ollama: `ollama serve`

### **Se respostas estranhas:**
1. Ajuste temperatura no config.py (0.3 = mais conservador)
2. Troque modelo: `ollama pull llama2:3b`

### **Se muito lento:**
1. Use modelo menor: `llama2:3b`
2. Reduza max_tokens no config.py

## ğŸ‰ RESULTADO

O Kairo agora deve:
- âœ… Responder perguntas normalmente
- âœ… Manter conversaÃ§Ã£o fluida
- âœ… Expressar personalidade
- âœ… Evoluir atravÃ©s das interaÃ§Ãµes
- âœ… Funcionar de forma estÃ¡vel

**O problema de comunicaÃ§Ã£o foi RESOLVIDO!** ğŸš€

